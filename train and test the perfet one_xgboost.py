import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# ===== 注转   拽 =====

subjects = [f'S{i}' for i in range(2, 18)]  # S2 注 S17
base_path = '/Users/galshemesh/Downloads/WESAD/'

all_data = []

for subject in subjects:
    file_path = f'{base_path}{subject}/{subject}.pkl'
    try:
        with open(file_path, 'rb') as file:
            data = pickle.load(file, encoding='latin1')

        signals = data['signal']
        labels = data['label']

        nonzero_idx = np.where(labels != 0)[0][0]

        eda = signals['chest']['EDA'].flatten()[nonzero_idx:]
        temp = signals['chest']['Temp'].flatten()[nonzero_idx:]
        label = labels[nonzero_idx:]

        min_len = min(len(eda), len(temp), len(label))
        eda = eda[:min_len]
        temp = temp[:min_len]
        label = label[:min_len]

        df_subject = pd.DataFrame({

            'eda': eda,
            'temp': temp,
            'label': label,
            'subject': subject
        })

        df_subject = df_subject[df_subject['label'].isin([1, 2, 3])]
        df_subject['is_stress'] = df_subject['label'].apply(lambda x: 1 if x == 2 else 0)

        all_data.append(df_subject)

    except FileNotFoundError:
        print(f"锔 拽抓 住专: {subject} -  注...")

#   
df_all = pd.concat(all_data, ignore_index=True)

print("\n 砖 :")
print(df_all.head())

print("\n转驻转 住专住:")
print(df_all['is_stress'].value_counts())

print("\n专砖转 拽:")
print(df_all['subject'].unique())

# ===== 住驻转 驻爪'专 砖 (Feature Engineering) =====

df_all['delta_eda'] = df_all['eda'].diff().fillna(0)
df_all['delta_temp'] = df_all['temp'].diff().fillna(0)

df_all['eda_mean_5'] = df_all['eda'].rolling(window=5, min_periods=1).mean()
df_all['temp_mean_5'] = df_all['temp'].rolling(window=5, min_periods=1).mean()

df_all['eda_std_5'] = df_all['eda'].rolling(window=5, min_periods=1).std().fillna(0)
df_all['temp_std_5'] = df_all['temp'].rolling(window=5, min_periods=1).std().fillna(0)

# ===== 拽  住 =====

train_df = df_all[df_all['subject'] != 'S9']
test_df = df_all[df_all['subject'] == 'S9']

features = ['eda', 'temp', 'delta_eda', 'delta_temp', 'eda_mean_5', 'temp_mean_5', 'eda_std_5', 'temp_std_5']

X_train = train_df[features]
y_train = train_df['is_stress']

X_test = test_df[features]
y_test = test_df['is_stress']

# ===== 专爪 =====

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===== 转  XGBoost =====

xgb_model = XGBClassifier(
    n_estimators=150,
    max_depth=5,
    learning_rate=0.1,
    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# =====   注 注拽 =====

xgb_model.fit(
    X_train_scaled,
    y_train,
    eval_set=[(X_train_scaled, y_train), (X_test_scaled, y_test)],
    verbose=True
)

# ===== 砖专转 转爪转  =====

results = xgb_model.evals_result()

# ===== 爪专 Learning Curve =====

epochs = len(results['validation_0']['logloss'])
x_axis = range(epochs)

plt.figure()
plt.plot(x_axis, results['validation_0']['logloss'], label='Train Loss')
plt.plot(x_axis, results['validation_1']['logloss'], label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Log Loss')
plt.title('XGBoost Log Loss Over Epochs')
plt.legend()
plt.grid()
plt.show()

# =====  注 拽 S9 =====

y_pred = xgb_model.predict(X_test_scaled)

print("\n转爪转 注 拽 S9:")

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ===== 砖专转  =====
predictions_df = pd.DataFrame({'Real': y_test.values, 'Predicted': y_pred})
predictions_df.to_csv('predictions_S9.csv', index=False)
print("\n拽抓 'predictions_S9.csv' 砖专 爪.")

# ===== 砖专转   =====
joblib.dump(xgb_model, 'xgboost_model.pkl')
print(" 砖专 -'xgboost_model.pkl'.")
import pickle
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# ===== 注转   拽 =====

#  拽 -S2 注 S17
subjects = [f'S{i}' for i in range(2, 18)]

base_path = '/Users/galshemesh/Downloads/WESAD/'

all_data = []

for subject in subjects:
    file_path = f'{base_path}{subject}/{subject}.pkl'
    try:
        with open(file_path, 'rb') as file:
            data = pickle.load(file, encoding='latin1')

        signals = data['signal']
        labels = data['label']

        nonzero_idx = np.where(labels != 0)[0][0]

        eda = signals['chest']['EDA'].flatten()[nonzero_idx:]
        temp = signals['chest']['Temp'].flatten()[nonzero_idx:]
        label = labels[nonzero_idx:]

        min_len = min(len(eda), len(temp), len(label))
        eda = eda[:min_len]
        temp = temp[:min_len]
        label = label[:min_len]

        df_subject = pd.DataFrame({
            'eda': eda,
            'temp': temp,
            'label': label,
            'subject': subject
        })

        df_subject = df_subject[df_subject['label'].isin([1, 2, 3])]
        df_subject['is_stress'] = df_subject['label'].apply(lambda x: 1 if x == 2 else 0)

        all_data.append(df_subject)

    except FileNotFoundError:
        print(f"锔 拽抓 住专: {subject} -  注...")

#   
df_all = pd.concat(all_data, ignore_index=True)

print("\n  砖 :")
print(df_all.head())
print("\nМ 转驻转 住专住:")
print(df_all['is_stress'].value_counts())
print("\nМ 专砖转 拽:")
print(df_all['subject'].unique())

# ===== 住驻转 驻爪'专 砖 (Feature Engineering) =====

df_all['delta_eda'] = df_all['eda'].diff().fillna(0)
df_all['delta_temp'] = df_all['temp'].diff().fillna(0)

df_all['eda_mean_5'] = df_all['eda'].rolling(window=5, min_periods=1).mean()
df_all['temp_mean_5'] = df_all['temp'].rolling(window=5, min_periods=1).mean()

df_all['eda_std_5'] = df_all['eda'].rolling(window=5, min_periods=1).std().fillna(0)
df_all['temp_std_5'] = df_all['temp'].rolling(window=5, min_periods=1).std().fillna(0)

# ===== 拽  住 =====

#  注  拽 抓 -S9
train_df = df_all[df_all['subject'] != 'S9']
test_df = df_all[df_all['subject'] == 'S9']

# 专砖转 驻爪'专 砖砖
features = ['eda', 'temp', 'delta_eda', 'delta_temp', 'eda_mean_5', 'temp_mean_5', 'eda_std_5', 'temp_std_5']

X_train = train_df[features]
y_train = train_df['is_stress']

X_test = test_df[features]
y_test = test_df['is_stress']

# ===== 专爪 =====

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===== 转  XGBoost =====

xgb_model = XGBClassifier(
    n_estimators=150,
    max_depth=5,
    learning_rate=0.1,
    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

#  
xgb_model.fit(X_train_scaled, y_train)

# =====  =====

y_pred = xgb_model.predict(X_test_scaled)

# ===== 转爪转 =====

print("\n Confusion Matrix (拽 注 S9):")
print(confusion_matrix(y_test, y_pred))

print("\n Classification Report (拽 注 S9):")
print(classification_report(y_test, y_pred))
